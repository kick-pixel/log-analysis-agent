"""
Agentç¼–æ’å™¨ (LangGraphç‰ˆ)

è´Ÿè´£Agentçš„åˆå§‹åŒ–ã€å·¥å…·è°ƒç”¨å’Œå¯¹è¯ç®¡ç†
é‡‡ç”¨LangGraphæ¶æ„å®ç°ï¼Œæä¾›æ›´å¼ºçš„çŠ¶æ€ç®¡ç†å’Œå¯æ§æ€§

ä½œè€…: Log Analysis Team
"""

from src.storage_layer.vector_search import VectorSearchEngine
from src.storage_layer.keyword_search import KeywordSearchEngine
from src.agent_layer.tools.log_tools import (
    ALL_TOOLS,
    init_tools,
    query_logs_by_time_range,
    search_error_keywords,
    semantic_search_logs,
    filter_logs_by_tag,
    get_log_context,
    get_error_statistics
)
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import add_messages
from langgraph.graph import StateGraph, END
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage, ToolMessage
from langchain_openai import ChatOpenAI
import logging
import os
import yaml
import functools
from typing import List, Dict, Optional, Any, Union, TypedDict, Annotated
from pathlib import Path
from loguru import logger

# CRITICAL FIX: Remove all Loguru handlers to prevent KeyError from LangChain/LangGraph logging
# Loguru intercepts standard library logging and tries to format strings with braces
logger.remove()  # Remove all handlers
# Add back a simple handler without interception
logger.add(lambda msg: print(msg, end=""),
           format="{time:HH:mm:ss} | {level} | {message}")

# Disable Loguru's interception of standard library logging
logging.getLogger("langchain").setLevel(logging.WARNING)
logging.getLogger("langgraph").setLevel(logging.WARNING)


class AgentState(TypedDict):
    """AgentçŠ¶æ€å®šä¹‰"""
    messages: Annotated[list, add_messages]


class LogAnalysisAgent:
    """æ—¥å¿—åˆ†æAgentç¼–æ’å™¨ (LangGraphå®ç°)

    é›†æˆLLMã€å·¥å…·å’Œè®°å¿†ï¼Œæä¾›æ™ºèƒ½æ—¥å¿—åˆ†æèƒ½åŠ›
    ä½¿ç”¨LangGraph StateGraphç®¡ç†å¯¹è¯çŠ¶æ€
    """

    def __init__(
        self,
        config_path: str = "./config/config.yaml",
        db_path: str = "./data/logs.db",
        vector_db_path: str = "./data/chroma_db"
    ):
        """åˆå§‹åŒ–Agent

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            db_path: SQLiteæ•°æ®åº“è·¯å¾„
            vector_db_path: ChromaDBè·¯å¾„
        """
        # åŠ è½½é…ç½®
        self.config = self._load_config(config_path)

        # åˆå§‹åŒ–å­˜å‚¨å¼•æ“
        logger.info("Initializing storage engines (LangGraph)...")
        self.keyword_engine = KeywordSearchEngine(db_path=db_path)
        self.vector_engine = VectorSearchEngine(db_path=vector_db_path)

        # å½“å‰ä¼šè¯IDï¼ˆç”¨äºæŸ¥è¯¢æ—¶è¿‡æ»¤ï¼‰
        self.current_session_id = None

        # åˆå§‹åŒ–å·¥å…·
        init_tools(self.keyword_engine, self.vector_engine, self)

        # åˆå§‹åŒ–LLM
        logger.info("Initializing LLM...")
        self.llm = self._init_llm()

        # ç»‘å®šå·¥å…·åˆ°LLM
        self.llm_with_tools = self.llm.bind_tools(ALL_TOOLS)

        # åˆ›å»ºAgent Graph
        logger.info("Creating LangGraph agent...")
        self.graph = self._create_graph()

        logger.info("LogAnalysisAgent (Graph) initialized successfully")

    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"Loaded configuration from {config_path}")
            return config
        except Exception as e:
            logger.warning(f"Failed to load config from {config_path}: {e}")
            logger.info("Using default configuration")
            return {
                'llm': {
                    'model': 'gpt-4o',
                    'temperature': 0.1,
                    'max_tokens': 4000
                },
                'agent': {
                    'max_iterations': 10,
                    'verbose': True
                }
            }

    def _init_llm(self) -> ChatOpenAI:
        """åˆå§‹åŒ–LLM"""
        llm_config = self.config.get('llm', {})

        # ä»ç¯å¢ƒå˜é‡è·å–API Key
        api_key = os.getenv('OPENAI_API_KEY')
        base_url = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
        model = os.getenv('OPENAI_MODEL')
        if not api_key:
            raise ValueError(
                "OPENAI_API_KEY not found in environment variables. "
                "Please set it in .env file or environment."
            )

        return ChatOpenAI(
            model=model or llm_config.get('model', 'gpt-4o'),
            temperature=llm_config.get('temperature', 0.1),
            max_tokens=llm_config.get('max_tokens', 4000),
            api_key=api_key,
            base_url=base_url
        )

    # --- Node Functions ---

    def call_model(self, state: AgentState):
        """è°ƒç”¨æ¨¡å‹èŠ‚ç‚¹"""
        messages = state['messages']
        response = self.llm_with_tools.invoke(messages)
        return {"messages": [response]}

    def node_fallback_to_semantic(self, state: AgentState):
        """FallbackèŠ‚ç‚¹ï¼šå°†å¤±è´¥çš„å…³é”®è¯æœç´¢è½¬æ¢ä¸ºè¯­ä¹‰æœç´¢"""
        import uuid
        try:
            messages = state['messages']

            # è·å–ä¸Šä¸€ä¸ªAIMessageï¼ˆå‘èµ·å…³é”®è¯æœç´¢çš„é‚£ä¸ªï¼‰
            last_ai_message = messages[-2]

            if not hasattr(last_ai_message, 'tool_calls') or not last_ai_message.tool_calls:
                return {"messages": [AIMessage(content="æ— æ³•æ‰§è¡ŒFallbackï¼šæœªæ‰¾åˆ°åŸå§‹å·¥å…·è°ƒç”¨")]}

            tool_call = last_ai_message.tool_calls[0]
            print(f"DEBUG: Tool Call Args: {tool_call.get('args')}")

            # å°è¯•è·å– keywordsï¼Œå…¼å®¹ä¸åŒçš„å‚æ•°åï¼ˆæœ‰äº›æ¨¡å‹å¯èƒ½ä¼šç”¨ query ç”šè‡³å…¶ä»–ï¼‰
            keywords = tool_call['args'].get('keywords')
            if not keywords:
                keywords = tool_call['args'].get('query')
            if not keywords:
                # æœ€åçš„å°è¯•ï¼šå–ç¬¬ä¸€ä¸ªå‚æ•°å€¼
                if tool_call['args']:
                    keywords = list(tool_call['args'].values())[0]

            if not keywords:
                return {"messages": [AIMessage(content="æ— æ³•æ‰§è¡ŒFallbackï¼šæ— æ³•æå–æœç´¢å…³é”®è¯")]}

            print(
                f"Using fallback: Keyword search failed for '{keywords}', trying semantic search.")

            # æ„é€ æ–°çš„AIMessageï¼Œè°ƒç”¨semantic_search_logs
            new_tool_call_id = str(uuid.uuid4())
            new_tool_call = {
                'name': 'semantic_search_logs',
                'args': {'query': str(keywords)},
                'id': new_tool_call_id,
                'type': 'tool_call'
            }

            return {"messages": [AIMessage(content=f"å…³é”®è¯ '{keywords}' æœªæœç´¢åˆ°ç»“æœï¼Œå°è¯•ä½¿ç”¨è¯­ä¹‰æœç´¢...", tool_calls=[new_tool_call])]}

        except Exception as e:
            logger.error(
                f"Error in node_fallback_to_semantic: {e}", exc_info=True)
            return {"messages": [AIMessage(content=f"Fallbackæ‰§è¡Œå‡ºé”™: {str(e)}")]}

    # --- Routing Logic ---

    def route_tools(self, state: AgentState) -> str:
        """è·¯ç”±é€»è¾‘ï¼šå†³å®šä¸‹ä¸€æ­¥æ˜¯è°ƒç”¨å·¥å…·è¿˜æ˜¯ç»“æŸ"""
        messages = state['messages']
        last_message = messages[-1]

        # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸ
        if not hasattr(last_message, "tool_calls") or not last_message.tool_calls:
            return "end"

        tool_call = last_message.tool_calls[0]
        tool_name = tool_call['name']

        # æ˜ å°„å·¥å…·ååˆ°èŠ‚ç‚¹å
        tool_node_map = {
            "query_logs_by_time_range": "node_time_query",
            "search_error_keywords": "node_search_keywords",
            "semantic_search_logs": "node_semantic_search",
            "filter_logs_by_tag": "node_filter",
            "get_log_context": "node_context",
            "get_error_statistics": "node_stats"
        }

        node_name = tool_node_map.get(tool_name)

        if node_name:
            return node_name

        logger.warning(f"Unknown tool called: {tool_name}, stopping.")
        return "end"

    def check_search_result(self, state: AgentState) -> str:
        """æ£€æŸ¥æœç´¢ç»“æœï¼Œå†³å®šæ˜¯å¦Fallback"""
        messages = state['messages']
        last_message = messages[-1]  # ToolMessage

        # æ£€æŸ¥å·¥å…·è¾“å‡º
        # å¦‚æœåŒ…å« "æ²¡æœ‰æ‰¾åˆ°" (log_tools.py ä¸­çš„æ ‡å‡†å›å¤)ï¼Œåˆ™è®¤ä¸ºæœç´¢å¤±è´¥
        if "æ²¡æœ‰æ‰¾åˆ°" in last_message.content or "found 0" in last_message.content.lower():
            return "fallback"

        return "agent"

    def _create_graph(self):
        """åˆ›å»ºLangGraphå›¾ (ä½¿ç”¨æ‹†åˆ†çš„Node + Smart Fallback)

        Returns:
            CompiledGraph: ç¼–è¯‘åçš„LangGraphå¯¹è±¡
        """
        # è·å–System Prompt
        agent_config = self.config.get('agent', {})
        system_prompt = agent_config.get('system_prompt', """
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è½¦è½½ç³»ç»Ÿï¼ˆAndroid/Linuxï¼‰æ—¥å¿—åˆ†æä¸“å®¶ï¼Œæ‹¥æœ‰15å¹´çš„æ•…éšœæ’æŸ¥ç»éªŒã€‚
ä½ æ“…é•¿åˆ†æAndroid Logcatã€Kernel Logç­‰å¤šç§æ—¥å¿—æ ¼å¼ã€‚

ä½ çš„å·¥ä½œæµç¨‹ï¼š
1. ç†è§£ç”¨æˆ·çš„é—®é¢˜æè¿°ï¼ˆæ•…éšœç°è±¡ã€å‘ç”Ÿæ—¶é—´ï¼‰
2. ä½¿ç”¨å·¥å…·æ£€ç´¢ç›¸å…³æ—¥å¿—ï¼ˆæ—¶é—´èŒƒå›´ã€å…³é”®è¯ã€æ¨¡å—ï¼‰
3. å®šä½å…³é”®é”™è¯¯æ—¥å¿—å’Œå †æ ˆä¿¡æ¯
4. åˆ†æä¸Šä¸‹æ–‡ï¼Œæ¨æ–­æ ¹æœ¬åŸå› 
5. ç»™å‡ºæ¸…æ™°çš„ç»“è®ºå’Œå»ºè®®

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- **æ•…éšœæ—¶é—´ç‚¹**ï¼šç²¾ç¡®åˆ°ç§’
- **å…³é”®æ—¥å¿—**ï¼šå±•ç¤ºæ ¸å¿ƒé”™è¯¯ä¿¡æ¯
- **æ ¹å› åˆ†æ**ï¼šè§£é‡Šä¸ºä»€ä¹ˆå‘ç”Ÿæ•…éšœ
- **å»ºè®®æ–¹æ¡ˆ**ï¼šç»™å‡ºå¯è¡Œçš„ä¿®å¤å»ºè®®

æ³¨æ„ï¼šå¦‚æœæ— æ³•ç¡®å®šæ ¹æœ¬åŸå› ï¼Œè¯·æ˜ç¡®è¯´æ˜ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚
        """.strip())

        # åˆå§‹åŒ–å›¾
        workflow = StateGraph(AgentState)

        # 1. æ·»åŠ AgentèŠ‚ç‚¹
        workflow.add_node("agent", self.call_model)

        # 2. æ·»åŠ å·¥å…·èŠ‚ç‚¹ - ç›´æ¥ä½¿ç”¨ToolNode
        workflow.add_node("node_time_query", ToolNode(
            [query_logs_by_time_range]))
        workflow.add_node("node_search_keywords",
                          ToolNode([search_error_keywords]))
        workflow.add_node("node_semantic_search",
                          ToolNode([semantic_search_logs]))
        workflow.add_node("node_filter", ToolNode([filter_logs_by_tag]))
        workflow.add_node("node_context", ToolNode([get_log_context]))
        workflow.add_node("node_stats", ToolNode([get_error_statistics]))

        # 3. æ·»åŠ FallbackèŠ‚ç‚¹
        workflow.add_node("node_fallback_to_semantic",
                          self.node_fallback_to_semantic)

        # 4. è®¾ç½®å…¥å£
        workflow.set_entry_point("agent")

        # 5. æ·»åŠ æ¡ä»¶è¾¹ (è·¯ç”±)
        workflow.add_conditional_edges(
            "agent",
            self.route_tools,
            {
                "node_time_query": "node_time_query",
                "node_search_keywords": "node_search_keywords",
                "node_semantic_search": "node_semantic_search",
                "node_filter": "node_filter",
                "node_context": "node_context",
                "node_stats": "node_stats",
                "end": END
            }
        )

        # 6. æ·»åŠ Smart Edges (Fallbacké€»è¾‘)
        # node_search_keywords -> check_search_result -> [agent, node_fallback_to_semantic]
        workflow.add_conditional_edges(
            "node_search_keywords",
            self.check_search_result,
            {
                "agent": "agent",
                "fallback": "node_fallback_to_semantic"
            }
        )

        # FallbackèŠ‚ç‚¹ç”Ÿæˆsemantic searchè°ƒç”¨ï¼Œç›´æ¥è¿å‘semantic searchå·¥å…·èŠ‚ç‚¹
        workflow.add_edge("node_fallback_to_semantic", "node_semantic_search")

        # 7. æ·»åŠ å…¶ä»–æ™®é€šè¾¹ (å›å½’)
        # æ³¨æ„ï¼šnode_search_keywords å·²ç»åœ¨ä¸Šé¢å¤„ç†äº†ï¼ˆå®ƒæ˜¯conditional edgeçš„èµ·ç‚¹ï¼‰
        workflow.add_edge("node_time_query", "agent")
        # node_search_keywords -> conditional check -> agent OR fallback
        workflow.add_edge("node_semantic_search", "agent")
        workflow.add_edge("node_filter", "agent")
        workflow.add_edge("node_context", "agent")
        workflow.add_edge("node_stats", "agent")

        # 8. ç¼–è¯‘
        checkpointer = MemorySaver()
        return workflow.compile(checkpointer=checkpointer)

    def analyze(
        self,
        query: str,
        chat_history: Optional[List] = None
    ) -> Dict:
        """åˆ†æç”¨æˆ·æŸ¥è¯¢

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            chat_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰

        Returns:
            åŒ…å«å›ç­”å’Œä¸­é—´æ­¥éª¤çš„å­—å…¸
        """
        try:
            print(f"Processing query: {query}")

            # ç¡®å®šçº¿ç¨‹ID (ç”¨äºè®°å¿†)
            thread_id = self.current_session_id or "default"
            config = {
                "configurable": {"thread_id": thread_id},
                "recursion_limit": 50  # Increase recursion limit to handle complex queries
            }

            # æ„å»ºè¾“å…¥æ¶ˆæ¯
            # å¦‚æœæ˜¯æ–°ä¼šè¯çš„ç¬¬ä¸€æ¬¡äº¤äº’ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦æ³¨å…¥SystemMessage
            # è¿™é‡Œæˆ‘ä»¬ç®€å•åˆ¤æ–­ï¼šå¦‚æœæ˜¯ç¬¬ä¸€æ¬¡ï¼ŒMemorySaveré‡Œä¹Ÿæ²¡ä¸œè¥¿ï¼Œæˆ‘ä»¬å…ˆå‘SystemMessage?
            # å®é™…ä¸ŠLangGraphçš„çŠ¶æ€æ˜¯æŒä¹…åŒ–çš„ï¼Œæˆ‘ä»¬å¯ä»¥æ¯æ¬¡éƒ½æŠŠSystemMessageä½œä¸ºç¬¬ä¸€ä¸ªæ¶ˆæ¯ä¼ è¿›å»å—ï¼Ÿ
            # æ›´å¥½çš„åšæ³•æ˜¯è®©LLM bind toolsæ—¶å¦‚æœä¸åŒ…å«system promptï¼Œå°±åœ¨è¿™é‡ŒåŠ 

            # è·å–System Prompt
            agent_config = self.config.get('agent', {})
            system_prompt_content = agent_config.get(
                'system_prompt', "You are a helpful assistant.")

            # è·å–å½“å‰çŠ¶æ€
            current_state = self.graph.get_state(config)

            input_messages = []
            # Only add SystemMessage if this is a new conversation (empty state)
            if not current_state.values or len(current_state.values.get('messages', [])) == 0:
                input_messages.append(SystemMessage(
                    content=system_prompt_content))

            input_messages.append(HumanMessage(content=query))

            # ä½¿ç”¨ stream è·å–èŠ‚ç‚¹æ‰§è¡Œä¿¡æ¯
            logger.info("="*80)
            logger.info("å¼€å§‹æ‰§è¡Œ Graph...")
            logger.info("="*80)

            final_messages = []
            for event in self.graph.stream(
                {"messages": input_messages},
                config=config,
                stream_mode="updates"  # è·å–æ¯ä¸ªèŠ‚ç‚¹çš„æ›´æ–°
            ):
                # event æ˜¯ä¸€ä¸ªå­—å…¸: {node_name: node_output}
                for node_name, node_output in event.items():
                    # è®°å½•èŠ‚ç‚¹æ‰§è¡Œ
                    logger.info(
                        f"======================== Node: {node_name} ========================")

                    if isinstance(node_output, dict):
                        # æ‰“å°æ›´æ–°çš„é”®
                        logger.info(
                            f"  Updated keys: {list(node_output.keys())}")

                        # å¦‚æœæœ‰ messagesï¼Œè®°å½•æ¶ˆæ¯æ•°é‡
                        if 'messages' in node_output:
                            messages = node_output['messages']
                            logger.info(f"  Added {len(messages)} message(s)")

                            # è®°å½•æœ€åä¸€æ¡æ¶ˆæ¯çš„ç±»å‹å’Œç®€è¦å†…å®¹
                            if messages:
                                last_msg = messages[-1]
                                msg_type = type(last_msg).__name__
                                logger.info(f"  Last message type: {msg_type}")

                                # å¦‚æœæœ‰ tool_callsï¼Œè®°å½•å·¥å…·å
                                if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:
                                    tool_names = [
                                        tc.get('name', 'unknown') for tc in last_msg.tool_calls]
                                    logger.info(
                                        f"  Tool calls: {', '.join(tool_names)}")

                                # å¦‚æœæœ‰å†…å®¹ï¼Œè®°å½•é¢„è§ˆ
                                if hasattr(last_msg, 'content') and last_msg.content:
                                    content_preview = str(last_msg.content)[
                                        :100].replace('\n', ' ')
                                    logger.info(
                                        f"  Content preview: {content_preview}...")
                    else:
                        logger.info(f"  Update: {node_output}")

                    logger.info("="*70)

            logger.info("Graph æ‰§è¡Œå®Œæˆ")
            logger.info("="*80)

            # è·å–å®Œæ•´çš„æœ€ç»ˆçŠ¶æ€
            complete_state = self.graph.get_state(config)
            final_messages = complete_state.values.get("messages", [])

            # æå–ç»“æœ
            answer = ""
            if final_messages:
                last_msg = final_messages[-1]
                if isinstance(last_msg, AIMessage):
                    answer = last_msg.content

            return {
                'answer': answer,
                'messages': final_messages,
                'success': True
            }

        except Exception as e:
            print(f"ERROR: Analysis error: {e}")
            return {
                'answer': f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                'messages': [],
                'success': False,
                'error': str(e)
            }

    def load_logs(
        self,
        log_file_path: str,
        session_id: str = "default"
    ) -> Dict:
        """åŠ è½½æ—¥å¿—æ–‡ä»¶
        (ä»£ç å¤ç”¨è‡ªorchestrator.pyï¼Œé€»è¾‘ä¸€è‡´)

        Args:
            log_file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            session_id: ä¼šè¯ID

        Returns:
            åŠ è½½ç»“æœå­—å…¸
        """
        from src.data_layer.parsers.logcat_parser import LogcatParser
        from src.data_layer.preprocessor import LogPreprocessor

        try:
            print(f"Loading log file: {log_file_path}")

            # è§£ææ—¥å¿—
            parser = LogcatParser()
            entries = parser.parse_file(log_file_path)

            if not entries:
                return {
                    'success': False,
                    'message': 'æœªèƒ½è§£æåˆ°æœ‰æ•ˆçš„æ—¥å¿—æ¡ç›®'
                }

            logger.info(f"Parsed {len(entries)} log entries")

            # é¢„å¤„ç†ï¼ˆä¿ç•™æ‰€æœ‰INFOåŠä»¥ä¸Šçº§åˆ«ï¼‰
            preprocessor = LogPreprocessor(
                enable_deduplication=True,
                enable_pii_masking=True,
                min_log_level='I'
            )
            processed_entries = preprocessor.process(entries)

            logger.info(f"Preprocessed to {len(processed_entries)} entries")

            # å­˜å…¥å…³é”®è¯æœç´¢å¼•æ“ï¼ˆç´¢å¼•æ‰€æœ‰æ—¥å¿—ï¼Œå…³é”®è¯æœç´¢å¾ˆå¿«ï¼‰
            self.keyword_engine.insert_logs(
                processed_entries, session_id=session_id)

            # å‘é‡æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–ï¼šåªç´¢å¼•ERRORå’ŒWARNçº§åˆ«æ—¥å¿—
            important_entries = [
                entry for entry in processed_entries
                if entry.level in ['W', 'E', 'F']  # WARN, ERROR, FATAL
            ]

            logger.info(
                f"Indexing {len(important_entries)} important logs (W/E/F) to vector database...")

            if important_entries:
                self.vector_engine.insert_logs(
                    important_entries, session_id=session_id)
            else:
                logger.warning(
                    "No ERROR/WARN logs found, skipping vector indexing")

            # ç»Ÿè®¡ä¿¡æ¯
            total_logs = len(processed_entries)
            vector_logs = len(important_entries)
            logger.info(
                f"ğŸ“Š å­˜å‚¨ç»Ÿè®¡: å…³é”®è¯ç´¢å¼•={total_logs}, å‘é‡ç´¢å¼•={vector_logs} ({vector_logs/total_logs*100:.1f}%)")

            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = self.keyword_engine.get_statistics(session_id=session_id)

            # è®¾ç½®å½“å‰ä¼šè¯IDï¼ˆç”¨äºåç»­æŸ¥è¯¢ï¼‰
            self.current_session_id = session_id
            print(f"âœ… Current session set to: {session_id}")

            return {
                'success': True,
                'message': f'æˆåŠŸåŠ è½½ {len(processed_entries)} æ¡æ—¥å¿—',
                'statistics': stats
            }

        except Exception as e:
            print(f"ERROR: Failed to load logs: {e}")
            return {
                'success': False,
                'message': f'åŠ è½½æ—¥å¿—å¤±è´¥: {str(e)}',
                'error': str(e)
            }

    def get_statistics(self, session_id: Optional[str] = None) -> Dict:
        """è·å–æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯"""
        return self.keyword_engine.get_statistics(session_id=session_id)

    def clear_session(self, session_id: str):
        """æ¸…é™¤ä¼šè¯æ•°æ®"""
        print(f"Clearing session: {session_id}")
        self.keyword_engine.clear_session(session_id)
        self.vector_engine.clear_session(session_id)


def main():
    """æµ‹è¯•å‡½æ•°"""
    from dotenv import load_dotenv

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    # æ£€æŸ¥API Key
    if not os.getenv('OPENAI_API_KEY'):
        print("\nâš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°OPENAI_API_KEYç¯å¢ƒå˜é‡")
        print("è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®OPENAI_API_KEY")
        print("\nè·³è¿‡Agentæµ‹è¯•ï¼Œä»…æµ‹è¯•æ—¥å¿—åŠ è½½åŠŸèƒ½\n")
        return

    # æµ‹è¯•æ ·æœ¬è·¯å¾„
    sample_path = Path(__file__).parent.parent.parent / \
        "tests" / "sample_logs" / "android_logcat_sample.log"

    # æ£€æŸ¥æµ‹è¯•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not sample_path.exists():
        print(f"\nâš ï¸  æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {sample_path}")
        return

    # åˆ›å»ºAgent
    print("\n=== åˆå§‹åŒ–Log Analysis Agent (LangGraphç‰ˆ) ===")
    agent = LogAnalysisAgent(
        db_path="./data/test_graph_agent_logs.db",
        vector_db_path="./data/test_graph_agent_chroma"
    )

    # åŠ è½½æ—¥å¿—
    print("\n=== åŠ è½½æµ‹è¯•æ—¥å¿— ===")

    # Clear previous session to avoid message accumulation
    try:
        agent.clear_session("test_graph_session")
        print("âœ… Cleared previous session state")
    except:
        pass

    load_result = agent.load_logs(
        str(sample_path), session_id="test_graph_session")
    print(f"åŠ è½½ç»“æœ: {load_result['message']}")
    if load_result['success']:
        stats = load_result['statistics']
        print(f"æ€»æ—¥å¿—æ•°: {stats.get('total_count', 0)}")
        print(f"çº§åˆ«åˆ†å¸ƒ: {stats.get('level_distribution', {})}")

    # æµ‹è¯•æŸ¥è¯¢1
    print("\n=== æµ‹è¯•æŸ¥è¯¢1: æŸ¥æ‰¾å´©æºƒ ===")
    result1 = agent.analyze("æŸ¥æ‰¾æ‰€æœ‰å´©æºƒ(Crash)ç›¸å…³çš„æ—¥å¿—")
    print(f"\nAgentå›ç­”:\n{result1['answer']}\n")

    # æµ‹è¯•æŸ¥è¯¢2
    print("\n=== æµ‹è¯•æŸ¥è¯¢2: åˆ†ææ—¶é—´æ®µ ===")
    result2 = agent.analyze("åˆ†æ14:00åˆ°14:30ä¹‹é—´çš„é”™è¯¯")
    print(f"\nAgentå›ç­”:\n{result2['answer']}\n")

    # æµ‹è¯•æŸ¥è¯¢3: æµ‹è¯•Fallback
    print("\n=== æµ‹è¯•æŸ¥è¯¢3: Search Fallback (Keyword -> Semantic) ===")
    # è¿™é‡Œçš„å…³é”®è¯ 'weird_glitch_888' è‚¯å®šä¸å­˜åœ¨ï¼Œåº”è¯¥è§¦å‘Fallback
    result3 = agent.analyze("å¸®æˆ‘æŸ¥æ‰¾åŒ…å« 'weird_glitch_888' çš„æ—¥å¿—ï¼Œæˆ–è€…ä»»ä½•çœ‹èµ·æ¥å¥‡æ€ªçš„é”™è¯¯")
    print(f"\nAgentå›ç­”:\n{result3['answer']}\n")

    # æ£€æŸ¥ä¸­é—´æ­¥éª¤æ˜¯å¦åŒ…å«Fallback
    fallback_occurred = False
    for msg in result3['messages']:
        if isinstance(msg, AIMessage) and "å°è¯•ä½¿ç”¨è¯­ä¹‰æœç´¢" in msg.content:
            fallback_occurred = True
            break

    if fallback_occurred:
        print("âœ… æ£€æµ‹åˆ°Fallbackæœºåˆ¶æˆåŠŸè§¦å‘ï¼")
    else:
        print("âŒ æœªæ£€æµ‹åˆ°Fallbackè§¦å‘ã€‚")


if __name__ == "__main__":
    main()
