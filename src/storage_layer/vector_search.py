"""
å‘é‡è¯­ä¹‰æ£€ç´¢å¼•æ“ (åŸºäºChromaDB)

åŠŸèƒ½:
1. å°†æ—¥å¿—è½¬æ¢ä¸ºè¯­ä¹‰å‘é‡
2. æ”¯æŒæ¨¡ç³Šè¯­ä¹‰æœç´¢
3. è¡¥å……å…³é”®è¯æœç´¢ï¼ˆå½“ç”¨æˆ·æè¿°ä¸ç²¾ç¡®æ—¶ï¼‰
4. çŸ¥è¯†åº“ç›¸ä¼¼åº¦åŒ¹é…

ä½œè€…: Log Analysis Team
"""

import chromadb
from chromadb.config import Settings
from typing import List, Dict, Optional
from pathlib import Path
from loguru import logger
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

from src.data_layer.parsers.logcat_parser import LogEntry


class VectorSearchEngine:
    """åŸºäºChromaDBçš„å‘é‡è¯­ä¹‰æ£€ç´¢å¼•æ“
    
    ä½¿ç”¨Embeddingæ¨¡å‹å°†æ—¥å¿—è½¬æ¢ä¸ºå‘é‡ï¼Œæ”¯æŒè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
    """
    
    def __init__(
        self,
        db_path: str = "./data/chroma_db",
        collection_name: str = "log_embeddings"
    ):
        """åˆå§‹åŒ–å‘é‡æœç´¢å¼•æ“
        
        Args:
            db_path: ChromaDBæ•°æ®åº“è·¯å¾„
            collection_name: é›†åˆåç§°
        """
        self.db_path = db_path
        self.collection_name = collection_name
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        Path(db_path).parent.mkdir(parents=True, exist_ok=True)
        
        try:
            # åˆ›å»ºChromaDBå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨æ–°APIï¼‰
            self.client = chromadb.PersistentClient(path=db_path)
            
            # è·å–æˆ–åˆ›å»ºé›†åˆ
            # ä½¿ç”¨é»˜è®¤çš„sentence-transformers embeddingæ¨¡å‹
            # HNSWå‚æ•°ä¼˜åŒ–ï¼šæå‡æŸ¥è¯¢å’Œå†™å…¥æ€§èƒ½
            self.collection = self.client.get_or_create_collection(
                name=collection_name,
                metadata={
                    "description": "Log embeddings for semantic search",
                    "hnsw:space": "cosine",  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
                    "hnsw:construction_ef": 100,  # æ„å»ºæ—¶çš„efå‚æ•°ï¼ˆå¹³è¡¡æ€§èƒ½å’Œè´¨é‡ï¼‰
                    "hnsw:M": 16  # HNSWå›¾çš„è¿æ¥æ•°ï¼ˆè¶Šå¤§è¶Šç²¾ç¡®ä½†è¶Šæ…¢ï¼‰
                }
            )
            
            logger.info(f"VectorSearchEngine initialized (db={db_path}, collection={collection_name})")
            logger.info(f"Collection currently has {self.collection.count()} documents")
            
        except Exception as e:
            logger.error(f"Failed to initialize ChromaDB: {e}")
            raise
    
    def _create_document(self, entry: LogEntry) -> str:
        """å°†æ—¥å¿—æ¡ç›®è½¬æ¢ä¸ºæ–‡æ¡£å­—ç¬¦ä¸²
        
        Args:
            entry: æ—¥å¿—æ¡ç›®
            
        Returns:
            ç”¨äºembeddingçš„æ–‡æ¡£å­—ç¬¦ä¸²
        """
        # ç»„åˆTagå’ŒMessageä½œä¸ºæ–‡æ¡£å†…å®¹
        # è¿™æ ·å¯ä»¥åŒæ—¶è€ƒè™‘æ¨¡å—å’Œå…·ä½“å†…å®¹çš„è¯­ä¹‰
        return f"{entry.tag}: {entry.message}"
    
    def _create_metadata(self, entry: LogEntry, session_id: str) -> Dict:
        """åˆ›å»ºå…ƒæ•°æ®å­—å…¸
        
        Args:
            entry: æ—¥å¿—æ¡ç›®
            session_id: ä¼šè¯ID
            
        Returns:
            å…ƒæ•°æ®å­—å…¸
        """
        return {
            'timestamp': entry.timestamp,
            'datetime': entry.datetime_obj.isoformat() if entry.datetime_obj else None,
            'level': entry.level,
            'tag': entry.tag,
            'line_number': entry.line_number,
            'session_id': session_id
        }
    
    def insert_logs(
        self,
        entries: List[LogEntry],
        session_id: str = "default",
        batch_size: int = 2000
    ) -> int:
        """æ‰¹é‡æ’å…¥æ—¥å¿—ï¼ˆè½¬æ¢ä¸ºå‘é‡ï¼‰- ä¼˜åŒ–ç‰ˆ
        
        æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼š
        1. âœ… å¢å¤§batch_sizeåˆ°2000ï¼ˆå‡å°‘æ‰¹æ¬¡æ•°å’ŒAPIè°ƒç”¨ï¼‰
        2. âœ… æ·»åŠ è¯¦ç»†çš„è¿›åº¦æ˜¾ç¤ºå’Œæ€§èƒ½ç›‘æ§
        3. âš ï¸  ç“¶é¢ˆåœ¨embeddingç”Ÿæˆï¼ˆsentence-transformersæ¨¡å‹ï¼‰
        
        æ€§èƒ½è¯´æ˜ï¼š
        - 38000æ¡æ—¥å¿—é¢„è®¡è€—æ—¶ï¼š5-10åˆ†é’Ÿï¼ˆå–å†³äºCPUæ€§èƒ½ï¼‰
        - ä¸»è¦æ—¶é—´æ¶ˆè€—åœ¨embeddingæ¨¡å‹è®¡ç®—å‘é‡ï¼ˆæ¯æ‰¹çº¦éœ€è¦60-70ç§’ï¼‰
        - å¦‚éœ€æ›´å¿«é€Ÿåº¦ï¼Œå»ºè®®è€ƒè™‘ï¼š
          a) ä½¿ç”¨æ›´å¿«çš„embeddingæ¨¡å‹ï¼ˆå¦‚OpenAI APIï¼‰
          b) ä½¿ç”¨GPUåŠ é€Ÿ
          c) å‡å°‘éœ€è¦ç´¢å¼•çš„æ—¥å¿—æ•°é‡ï¼ˆåªç´¢å¼•ERROR/WARNçº§åˆ«ï¼‰
        
        Args:
            entries: æ—¥å¿—æ¡ç›®åˆ—è¡¨
            session_id: ä¼šè¯ID
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤2000ï¼Œå»ºè®®1000-5000ï¼‰
            
        Returns:
            æ’å…¥çš„æ—¥å¿—æ¡æ•°
        """
        if not entries:
            logger.warning("No entries to insert")
            return 0
        
        start_time = time.time()
        total_batches = (len(entries) + batch_size - 1) // batch_size
        
        logger.info(f"")
        logger.info(f"{'='*70}")
        logger.info(f"âš¡ å¼€å§‹æ’å…¥æ—¥å¿—åˆ°å‘é‡æ•°æ®åº“")
        logger.info(f"{'='*70}")
        logger.info(f"ğŸ“Š æ€»æ¡æ•°: {len(entries):,} æ¡")
        logger.info(f"ğŸ“¦ æ‰¹æ¬¡é…ç½®: batch_size={batch_size:,}, total_batches={total_batches}")
        logger.info(f"â±ï¸  é¢„è®¡è€—æ—¶: {total_batches * 60 / 60:.1f}-{total_batches * 70 / 60:.1f} åˆ†é’Ÿ")
        logger.info(f"{'='*70}")
        
        # å‡†å¤‡æ•°æ®ï¼ˆé¢„å¤„ç†é˜¶æ®µï¼‰
        prep_start = time.time()
        documents = []
        metadatas = []
        ids = []
        
        for i, entry in enumerate(entries):
            # åˆ›å»ºæ–‡æ¡£
            doc = self._create_document(entry)
            documents.append(doc)
            
            # åˆ›å»ºå…ƒæ•°æ®
            metadata = self._create_metadata(entry, session_id)
            metadatas.append(metadata)
            
            # åˆ›å»ºå”¯ä¸€IDï¼ˆsession_id + line_numberï¼‰
            doc_id = f"{session_id}_{entry.line_number}"
            ids.append(doc_id)
        
        prep_time = time.time() - prep_start
        logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ: {prep_time:.2f}s")
        logger.info(f"")
        
        # åˆ†æ‰¹æ’å…¥ï¼ˆä¸²è¡Œå¤„ç†ï¼Œå› ä¸ºembeddingç”Ÿæˆæ˜¯ç“¶é¢ˆï¼‰
        total_inserted = 0
        failed_batches = []
        
        insert_start = time.time()
        
        for i in range(0, len(documents), batch_size):
            batch_num = i // batch_size + 1
            batch_docs = documents[i:i + batch_size]
            batch_meta = metadatas[i:i + batch_size]
            batch_ids = ids[i:i + batch_size]
            
            batch_start = time.time()
            
            try:
                self.collection.add(
                    documents=batch_docs,
                    metadatas=batch_meta,
                    ids=batch_ids
                )
                
                batch_time = time.time() - batch_start
                total_inserted += len(batch_docs)
                
                # è®¡ç®—è¿›åº¦å’Œé¢„ä¼°å‰©ä½™æ—¶é—´
                progress = (batch_num / total_batches) * 100
                avg_time_per_batch = (time.time() - insert_start) / batch_num
                remaining_batches = total_batches - batch_num
                eta_seconds = remaining_batches * avg_time_per_batch
                eta_minutes = eta_seconds / 60
                
                logger.info(
                    f"âœ… Batch {batch_num}/{total_batches} | "
                    f"{len(batch_docs):,} æ¡ | "
                    f"è€—æ—¶ {batch_time:.1f}s | "
                    f"é€Ÿåº¦ {len(batch_docs)/batch_time:.1f} æ¡/s | "
                    f"è¿›åº¦ {progress:.1f}% | "
                    f"é¢„è®¡å‰©ä½™ {eta_minutes:.1f}min"
                )
                
            except Exception as e:
                logger.error(f"âŒ Batch {batch_num} æ’å…¥å¤±è´¥: {e}")
                failed_batches.append((batch_num, str(e)))
        
        insert_time = time.time() - insert_start
        total_time = time.time() - start_time
        
        # ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"")
        logger.info(f"{'='*70}")
        logger.info(f"âœ¨ æ’å…¥å®Œæˆç»Ÿè®¡")
        logger.info(f"{'='*70}")
        logger.info(f"ğŸ“¥ æ€»æ¡æ•°: {len(entries):,} æ¡")
        logger.info(f"âœ… æˆåŠŸæ’å…¥: {total_inserted:,} æ¡ ({total_inserted/len(entries)*100:.1f}%)")
        logger.info(f"âŒ å¤±è´¥æ‰¹æ¬¡: {len(failed_batches)} ä¸ª")
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}s ({total_time/60:.2f} åˆ†é’Ÿ)")
        logger.info(f"   - æ•°æ®é¢„å¤„ç†: {prep_time:.2f}s ({prep_time/total_time*100:.1f}%)")
        logger.info(f"   - Embeddingç”Ÿæˆ+æ’å…¥: {insert_time:.2f}s ({insert_time/total_time*100:.1f}%)")
        logger.info(f"ğŸš€ å¹³å‡é€Ÿåº¦: {total_inserted/total_time:.1f} æ¡/ç§’")
        logger.info(f"{'='*70}")
        
        if failed_batches:
            logger.warning(f"ä»¥ä¸‹æ‰¹æ¬¡æ’å…¥å¤±è´¥:")
            for batch_num, error in failed_batches:
                logger.warning(f"  - Batch {batch_num}: {error}")
        
        return total_inserted
    
    def semantic_search(
        self,
        query: str,
        n_results: int = 10,
        level: Optional[str] = None,
        session_id: Optional[str] = None
    ) -> List[Dict]:
        """è¯­ä¹‰æœç´¢
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆè‡ªç„¶è¯­è¨€æè¿°ï¼‰
            n_results: è¿”å›ç»“æœæ•°é‡
            level: æ—¥å¿—çº§åˆ«è¿‡æ»¤ (å¯é€‰)
            session_id: ä¼šè¯IDè¿‡æ»¤ (å¯é€‰)
            
        Returns:
            åŒ¹é…çš„æ—¥å¿—åˆ—è¡¨ï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰
        """
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        where = {}
        if level:
            where['level'] = level
        if session_id:
            where['session_id'] = session_id
        
        try:
            # æ‰§è¡ŒæŸ¥è¯¢
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                where=where if where else None
            )
            
            # è§£æç»“æœ
            matched_logs = []
            if results and results['ids'] and len(results['ids']) > 0:
                for i, doc_id in enumerate(results['ids'][0]):
                    log_data = {
                        'id': doc_id,
                        'document': results['documents'][0][i],
                        'metadata': results['metadatas'][0][i],
                        'distance': results['distances'][0][i] if 'distances' in results else None
                    }
                    matched_logs.append(log_data)
            
            print(f"DEBUG: Semantic search for '{query}' returned {len(matched_logs)} results")
            return matched_logs
            
        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            return []
    
    def find_similar_logs(
        self,
        reference_log_id: str,
        n_results: int = 5
    ) -> List[Dict]:
        """æŸ¥æ‰¾ç›¸ä¼¼çš„æ—¥å¿—
        
        Args:
            reference_log_id: å‚è€ƒæ—¥å¿—çš„ID
            n_results: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            ç›¸ä¼¼æ—¥å¿—åˆ—è¡¨
        """
        try:
            # è·å–å‚è€ƒæ—¥å¿—
            ref_result = self.collection.get(ids=[reference_log_id])
            
            if not ref_result or not ref_result['documents']:
                logger.warning(f"Reference log {reference_log_id} not found")
                return []
            
            ref_document = ref_result['documents'][0]
            
            # æœç´¢ç›¸ä¼¼æ—¥å¿—
            results = self.collection.query(
                query_texts=[ref_document],
                n_results=n_results + 1  # +1 å› ä¸ºä¼šåŒ…å«è‡ªå·±
            )
            
            # è§£æç»“æœï¼ˆæ’é™¤è‡ªå·±ï¼‰
            similar_logs = []
            if results and results['ids'] and len(results['ids']) > 0:
                for i, doc_id in enumerate(results['ids'][0]):
                    if doc_id != reference_log_id:  # æ’é™¤è‡ªå·±
                        log_data = {
                            'id': doc_id,
                            'document': results['documents'][0][i],
                            'metadata': results['metadatas'][0][i],
                            'distance': results['distances'][0][i] if 'distances' in results else None
                        }
                        similar_logs.append(log_data)
            
            logger.info(f"Found {len(similar_logs)} similar logs for {reference_log_id}")
            return similar_logs[:n_results]  # é™åˆ¶è¿”å›æ•°é‡
            
        except Exception as e:
            logger.error(f"Find similar logs failed: {e}")
            return []
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_count = self.collection.count()
            
            # è·å–æ‰€æœ‰å…ƒæ•°æ®è¿›è¡Œç»Ÿè®¡
            all_data = self.collection.get()
            
            # ç»Ÿè®¡å„çº§åˆ«
            level_dist = {}
            session_dist = {}
            
            if all_data and all_data['metadatas']:
                for metadata in all_data['metadatas']:
                    level = metadata.get('level', 'Unknown')
                    level_dist[level] = level_dist.get(level, 0) + 1
                    
                    session = metadata.get('session_id', 'Unknown')
                    session_dist[session] = session_dist.get(session, 0) + 1
            
            return {
                'total_documents': total_count,
                'level_distribution': level_dist,
                'session_distribution': session_dist
            }
            
        except Exception as e:
            logger.error(f"Get statistics failed: {e}")
            return {
                'total_documents': 0,
                'level_distribution': {},
                'session_distribution': {}
            }
    
    def clear_session(self, session_id: str):
        """æ¸…é™¤æŒ‡å®šä¼šè¯çš„å‘é‡
        
        Args:
            session_id: ä¼šè¯ID
        """
        try:
            # è·å–è¯¥sessionçš„æ‰€æœ‰ID
            results = self.collection.get(
                where={'session_id': session_id}
            )
            
            if results and results['ids']:
                self.collection.delete(ids=results['ids'])
                logger.info(f"Cleared {len(results['ids'])} vectors for session: {session_id}")
            else:
                logger.info(f"No vectors found for session: {session_id}")
                
        except Exception as e:
            logger.error(f"Clear session failed: {e}")
    
    def reset(self):
        """é‡ç½®æ•´ä¸ªé›†åˆï¼ˆè°¨æ…ä½¿ç”¨ï¼‰"""
        try:
            self.client.delete_collection(self.collection_name)
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "Log embeddings for semantic search"}
            )
            logger.warning("Vector database has been reset")
        except Exception as e:
            logger.error(f"Reset failed: {e}")


def main():
    """æµ‹è¯•å‡½æ•°"""
    from src.data_layer.parsers.logcat_parser import LogcatParser
    from pathlib import Path
    
    # æµ‹è¯•æ ·æœ¬è·¯å¾„
    sample_path = Path(__file__).parent.parent.parent / "tests" / "sample_logs" / "android_logcat_sample.log"
    
    # è§£ææ—¥å¿—
    parser = LogcatParser()
    entries = parser.parse_file(str(sample_path))
    
    print(f"\nè§£æäº† {len(entries)} æ¡æ—¥å¿—")
    
    # åˆ›å»ºå‘é‡æœç´¢å¼•æ“
    vector_engine = VectorSearchEngine(db_path="./data/test_chroma_db")
    
    # æ¸…é™¤æ—§æ•°æ®
    vector_engine.clear_session("test_session")
    
    # æ’å…¥æ—¥å¿—
    print("\næ­£åœ¨æ’å…¥æ—¥å¿—å¹¶ç”Ÿæˆå‘é‡...")
    inserted = vector_engine.insert_logs(entries, session_id="test_session")
    print(f"æ’å…¥äº† {inserted} æ¡æ—¥å¿—")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = vector_engine.get_statistics()
    print("\n=== å‘é‡æ•°æ®åº“ç»Ÿè®¡ ===")
    print(f"æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
    print(f"çº§åˆ«åˆ†å¸ƒ: {stats['level_distribution']}")
    print(f"ä¼šè¯åˆ†å¸ƒ: {stats['session_distribution']}")
    
    # è¯­ä¹‰æœç´¢ï¼šæŸ¥æ‰¾å†…å­˜ç›¸å…³é—®é¢˜
    print("\n=== è¯­ä¹‰æœç´¢: 'å†…å­˜ä¸è¶³å¯¼è‡´å´©æºƒ' ===")
    memory_results = vector_engine.semantic_search(
        query="memory allocation failure crash",
        n_results=5
    )
    for i, result in enumerate(memory_results, 1):
        print(f"{i}. [{result['metadata']['timestamp']}] {result['metadata']['level']}/{result['metadata']['tag']}")
        print(f"   {result['document'][:100]}")
        if result['distance']:
            print(f"   ç›¸ä¼¼åº¦è·ç¦»: {result['distance']:.4f}")
    
    # è¯­ä¹‰æœç´¢ï¼šæŸ¥æ‰¾ç›¸æœºç›¸å…³é—®é¢˜
    print("\n=== è¯­ä¹‰æœç´¢: 'ç›¸æœºå¯åŠ¨å¤±è´¥' ===")
    camera_results = vector_engine.semantic_search(
        query="camera failed to start unable to open",
        n_results=5
    )
    for i, result in enumerate(camera_results, 1):
        print(f"{i}. [{result['metadata']['timestamp']}] {result['metadata']['level']}/{result['metadata']['tag']}")
        print(f"   {result['document'][:100]}")
        if result['distance']:
            print(f"   ç›¸ä¼¼åº¦è·ç¦»: {result['distance']:.4f}")
    
    # æŸ¥æ‰¾ç›¸ä¼¼æ—¥å¿—
    if camera_results:
        first_id = camera_results[0]['id']
        print(f"\n=== æŸ¥æ‰¾ä¸ç¬¬ä¸€æ¡ç›¸æœºæ—¥å¿—ç›¸ä¼¼çš„æ—¥å¿— ===")
        similar_results = vector_engine.find_similar_logs(first_id, n_results=3)
        for i, result in enumerate(similar_results, 1):
            print(f"{i}. [{result['metadata']['timestamp']}] {result['metadata']['level']}/{result['metadata']['tag']}")
            print(f"   {result['document'][:100]}")
            if result['distance']:
                print(f"   ç›¸ä¼¼åº¦è·ç¦»: {result['distance']:.4f}")


if __name__ == "__main__":
    main()

